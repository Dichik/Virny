{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Redirect to introduction/introduction.md</p>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/overview/#analyzers","title":"analyzers","text":"<p>Subgroup Error and Variance Analyzers.</p> <p>This module contains fairness and stability analysing methods for defined subgroups. The purpose of an analyzer is to analyse defined metrics for defined subgroups.</p> <ul> <li>AbstractOverallVarianceAnalyzer</li> <li>AbstractSubgroupAnalyzer</li> <li>BatchOverallVarianceAnalyzer</li> <li>SubgroupErrorAnalyzer</li> <li>SubgroupVarianceAnalyzer</li> <li>SubgroupVarianceCalculator</li> </ul>"},{"location":"api/overview/#configs","title":"configs","text":"<p>Configs amd constants for the source code logic.</p>"},{"location":"api/overview/#custom_classes","title":"custom_classes","text":"<p>This module contains custom classes for metrics computation interfaces. The purpose is to split metrics computation and visualization pipeline on components that are highly  customizable for future library features.</p> <ul> <li>BaseFlowDataset</li> <li>MetricsComposer</li> <li>MetricsVisualizer</li> </ul>"},{"location":"api/overview/#datasets","title":"datasets","text":"<p>This module contains sample datasets and data loaders. The purpose is to provide sample datasets for functionality testing and show examples of data loaders (aka dataset classes).</p> <ul> <li>ACSEmploymentDataset</li> <li>ACSIncomeDataset</li> <li>ACSMobilityDataset</li> <li>ACSPublicCoverageDataset</li> <li>ACSTravelTimeDataset</li> <li>CompasDataset</li> <li>CompasWithoutSensitiveAttrsDataset</li> <li>DiabetesDataset</li> <li>LawSchoolDataset</li> <li>RicciDataset</li> </ul>"},{"location":"api/overview/#incremental_ml","title":"incremental_ml","text":""},{"location":"api/overview/#metrics","title":"metrics","text":"<p>This module contains functions for computing subgroup variance and error metrics.</p> <ul> <li>compute_churn</li> <li>compute_conf_interval</li> <li>compute_entropy_from_predicted_probability</li> <li>compute_jitter</li> <li>compute_per_sample_accuracy</li> <li>compute_std_mean_iqr_metrics</li> </ul>"},{"location":"api/overview/#preprocessing","title":"preprocessing","text":"<p>Preprocessing techniques.</p> <p>This module contains function for input dataset preprocessing.</p> <ul> <li>get_dummies</li> <li>make_features_dfs</li> <li>preprocess_dataset</li> </ul>"},{"location":"api/overview/#user_interfaces","title":"user_interfaces","text":"<p>User interfaces.</p> <p>This module contains user interfaces for metrics computation.</p> <ul> <li>compute_metrics_multiple_runs_with_db_writer</li> <li>compute_metrics_multiple_runs_with_multiple_test_sets</li> <li>compute_metrics_with_config</li> <li>compute_model_metrics</li> <li>compute_model_metrics_with_config</li> <li>run_metrics_computation</li> </ul>"},{"location":"api/overview/#utils","title":"utils","text":"<p>Common helpers and utils.</p> <ul> <li>count_prediction_stats</li> <li>create_test_protected_groups</li> <li>validate_config</li> </ul>"},{"location":"api/analyzers/AbstractOverallVarianceAnalyzer/","title":"AbstractOverallVarianceAnalyzer","text":"<p>Abstract class for an analyzer that computes overall variance metrics for subgroups.</p>"},{"location":"api/analyzers/AbstractOverallVarianceAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>base_model</p> <p>Base model for stability measuring</p> </li> <li> <p>base_model_name (str)</p> <p>Model name like 'HoeffdingTreeClassifier' or 'LogisticRegression'</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>[0-1], fraction from train_pd_dataset for fitting an ensemble of base models</p> </li> <li> <p>X_train (pandas.core.frame.DataFrame)</p> <p>Processed features train set</p> </li> <li> <p>y_train (pandas.core.frame.DataFrame)</p> <p>Targets train set</p> </li> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>dataset_name (str)</p> <p>Name of dataset, used for correct results naming</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators in ensemble to measure base_model stability</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.  As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/analyzers/AbstractOverallVarianceAnalyzer/#methods","title":"Methods","text":"UQ_by_boostrap <p>Quantifying uncertainty of the base model by constructing an ensemble from bootstrapped samples.</p> <p>Return a dictionary where keys are models indexes, and values are lists of  correspondent model predictions for X_test set.</p> <p>Parameters</p> <ul> <li>boostrap_size     (int)    </li> <li>with_replacement     (bool)    </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> compute_metrics <p>Measure metrics for the base model. Display plots for analysis if needed. Save results to a .pkl file</p> <p>Parameters</p> <ul> <li>make_plots     (bool)     \u2013 defaults to <code>False</code> </li> <li>save_results     (bool)     \u2013 defaults to <code>True</code> </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> get_metrics_dict print_metrics save_metrics_to_file"},{"location":"api/analyzers/AbstractSubgroupAnalyzer/","title":"AbstractSubgroupAnalyzer","text":"<p>Abstract class for a subgroup analyzer to compute metrics for subgroups.</p>"},{"location":"api/analyzers/AbstractSubgroupAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these attributes</p> </li> <li> <p>test_protected_groups (dict)</p> <p>A dictionary where keys are sensitive attributes, and values input dataset rows  that are correspondent to these sensitive attributes</p> </li> <li> <p>computation_mode (str) \u2013 defaults to <code>None</code></p> <p>A mode to compute metrics. It can have two values 'error_analysis' and default (None).</p> </li> </ul>"},{"location":"api/analyzers/AbstractSubgroupAnalyzer/#methods","title":"Methods","text":"compute_subgroup_metrics <p>Compute metrics for each subgroup in self.test_protected_groups using _compute_metrics method.</p> <p>Return a dictionary where keys are subgroup names, and values are subgroup metrics.</p> <p>Parameters</p> <ul> <li>y_preds </li> <li>save_results     (bool)    </li> <li>result_filename     (str)     \u2013 defaults to <code>None</code> </li> <li>save_dir_path     (str)     \u2013 defaults to <code>None</code> </li> </ul> save_metrics_to_file <p>Parameters</p> <ul> <li>result_filename     (str)    </li> <li>save_dir_path     (str)    </li> </ul>"},{"location":"api/analyzers/BatchOverallVarianceAnalyzer/","title":"BatchOverallVarianceAnalyzer","text":"<p>Analyzer to compute subgroup variance metrics for batch learning models.</p>"},{"location":"api/analyzers/BatchOverallVarianceAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>base_model</p> <p>Base model for stability measuring</p> </li> <li> <p>base_model_name (str)</p> <p>Model name like 'HoeffdingTreeClassifier' or 'LogisticRegression'</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>[0-1], fraction from train_pd_dataset for fitting an ensemble of base models</p> </li> <li> <p>X_train (pandas.core.frame.DataFrame)</p> <p>Processed features train set</p> </li> <li> <p>y_train (pandas.core.frame.DataFrame)</p> <p>Targets train set</p> </li> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>target_column (str)</p> <p>Name of the target column</p> </li> <li> <p>dataset_name (str)</p> <p>Name of dataset, used for correct results naming</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators in ensemble to measure base_model stability</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.  As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/analyzers/BatchOverallVarianceAnalyzer/#methods","title":"Methods","text":"UQ_by_boostrap <p>Quantifying uncertainty of the base model by constructing an ensemble from bootstrapped samples.</p> <p>Return a dictionary where keys are models indexes, and values are lists of  correspondent model predictions for X_test set.</p> <p>Parameters</p> <ul> <li>boostrap_size     (int)    </li> <li>with_replacement     (bool)    </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> compute_metrics <p>Measure metrics for the base model. Display plots for analysis if needed. Save results to a .pkl file</p> <p>Parameters</p> <ul> <li>make_plots     (bool)     \u2013 defaults to <code>False</code> </li> <li>save_results     (bool)     \u2013 defaults to <code>True</code> </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> get_metrics_dict print_metrics save_metrics_to_file"},{"location":"api/analyzers/SubgroupErrorAnalyzer/","title":"SubgroupErrorAnalyzer","text":"<p>Analyzer to compute error metrics for subgroups.</p>"},{"location":"api/analyzers/SubgroupErrorAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these subgroups</p> </li> <li> <p>test_protected_groups (dict) \u2013 defaults to <code>None</code></p> <p>A dictionary where keys are sensitive attributes, and values input dataset rows  that are correspondent to these sensitive attributes.</p> </li> <li> <p>computation_mode (str) \u2013 defaults to <code>None</code></p> <p>[Optional] A non-default mode for metrics computation. Should be included in the ComputationMode enum.</p> </li> </ul>"},{"location":"api/analyzers/SubgroupErrorAnalyzer/#methods","title":"Methods","text":"compute_subgroup_metrics <p>Compute metrics for each subgroup in self.test_protected_groups using _compute_metrics method.</p> <p>Return a dictionary where keys are subgroup names, and values are subgroup metrics.</p> <p>Parameters</p> <ul> <li>y_preds </li> <li>save_results     (bool)    </li> <li>result_filename     (str)     \u2013 defaults to <code>None</code> </li> <li>save_dir_path     (str)     \u2013 defaults to <code>None</code> </li> </ul> save_metrics_to_file <p>Parameters</p> <ul> <li>result_filename     (str)    </li> <li>save_dir_path     (str)    </li> </ul>"},{"location":"api/analyzers/SubgroupVarianceAnalyzer/","title":"SubgroupVarianceAnalyzer","text":"<p>Analyzer to compute variance metrics for subgroups.</p>"},{"location":"api/analyzers/SubgroupVarianceAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>model_setting (virny.configs.constants.ModelSetting)</p> <p>Model learning type; a constant from virny.configs.constants.ModelSetting</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators for bootstrap</p> </li> <li> <p>base_model</p> <p>Initialized base model to analyze</p> </li> <li> <p>base_model_name (str)</p> <p>Model name</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>[0-1], fraction from train_pd_dataset for fitting an ensemble of base models</p> </li> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>Initialized object of GenericPipeline class</p> </li> <li> <p>dataset_name (str)</p> <p>Name of dataset, used for correct results naming</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attribute names (including attributes intersections),  and values are privilege values for these attributes</p> </li> <li> <p>test_protected_groups (dict)</p> <p>A dictionary of protected groups where keys are subgroup names,  and values are X_test row indexes correspondent to this subgroup.</p> </li> <li> <p>computation_mode (str) \u2013 defaults to <code>None</code></p> <p>[Optional] A non-default mode for metrics computation. Should be included in the ComputationMode enum.</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.  As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/analyzers/SubgroupVarianceAnalyzer/#methods","title":"Methods","text":"compute_metrics <p>Measure variance metrics for subgroups for the base model. Display variance plots for analysis if needed.  Save results to a .csv file if needed.</p> <p>Return averaged bootstrap predictions and a pandas dataframe of variance metrics for subgroups.</p> <p>Parameters</p> <ul> <li>save_results     (bool)    </li> <li>result_filename     (str)     \u2013 defaults to <code>None</code> </li> <li>save_dir_path     (str)     \u2013 defaults to <code>None</code> </li> <li>make_plots     (bool)     \u2013 defaults to <code>True</code> </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_test_protected_groups set_test_sets"},{"location":"api/analyzers/SubgroupVarianceCalculator/","title":"SubgroupVarianceCalculator","text":"<p>Calculator that calculates variance metrics for subgroups.</p>"},{"location":"api/analyzers/SubgroupVarianceCalculator/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these subgroups</p> </li> <li> <p>test_protected_groups \u2013 defaults to <code>None</code></p> <p>A dictionary where keys are sensitive attributes, and values input dataset rows  that are correspondent to these sensitive attributes.</p> </li> <li> <p>computation_mode (str) \u2013 defaults to <code>None</code></p> <p>[Optional] A non-default mode for metrics computation. Should be included in the ComputationMode enum.</p> </li> </ul>"},{"location":"api/analyzers/SubgroupVarianceCalculator/#methods","title":"Methods","text":"compute_subgroup_metrics <p>Compute variance metrics for subgroups.</p> <p>Return a dict of dicts where key is 'overall' or a subgroup name, and value is a dict of metrics for this subgroup.</p> <p>Parameters</p> <ul> <li>models_predictions     (dict)    </li> <li>save_results     (bool)    </li> <li>result_filename     (str)     \u2013 defaults to <code>None</code> </li> <li>save_dir_path     (str)     \u2013 defaults to <code>None</code> </li> </ul> save_metrics_to_file <p>Parameters</p> <ul> <li>result_filename     (str)    </li> <li>save_dir_path     (str)    </li> </ul> set_overall_variance_metrics"},{"location":"api/custom-classes/BaseFlowDataset/","title":"BaseFlowDataset","text":"<p>Dataset class with custom train and test splits that is used as input for metrics computation interfaces. Create your dataset class based on this one to use it for metrics computation interfaces.</p>"},{"location":"api/custom-classes/BaseFlowDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>init_features_df (pandas.core.frame.DataFrame)</p> <p>Full train + test non-preprocessed dataset of features without the target column.  It is used for creating test groups.</p> </li> <li> <p>X_train_val (pandas.core.frame.DataFrame)</p> <p>Train dataframe of features</p> </li> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Test dataframe of features</p> </li> <li> <p>y_train_val (pandas.core.frame.DataFrame)</p> <p>Train dataframe with a target column</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Test dataframe with a target column</p> </li> <li> <p>target (str)</p> <p>Name of the target column name</p> </li> <li> <p>numerical_columns (list)</p> <p>List of numerical column names</p> </li> <li> <p>categorical_columns (list)</p> <p>List of categorical column names</p> </li> </ul>"},{"location":"api/custom-classes/MetricsComposer/","title":"MetricsComposer","text":"<p>Composer class that combines different subgroup metrics to create group metrics  such as 'Disparate_Impact' or 'Accuracy_Parity'</p>"},{"location":"api/custom-classes/MetricsComposer/#parameters","title":"Parameters","text":"<ul> <li> <p>models_metrics_dct (dict)</p> <p>Dictionary where keys are model names and values are dataframes of subgroups metrics for each model</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attribute names (including attributes intersections),  and values are privilege values for these attributes</p> </li> </ul>"},{"location":"api/custom-classes/MetricsComposer/#methods","title":"Methods","text":"compose_metrics <p>Compose subgroup metrics from self.model_metrics_df.</p> <p>Return a dictionary of composed metrics.</p>"},{"location":"api/custom-classes/MetricsVisualizer/","title":"MetricsVisualizer","text":"<p>Class to create useful visualizations of models metrics.</p>"},{"location":"api/custom-classes/MetricsVisualizer/#parameters","title":"Parameters","text":"<ul> <li> <p>models_metrics_dct (dict)</p> <p>Dictionary where keys are model names and values are dataframes of subgroup metrics for each model</p> </li> <li> <p>models_composed_metrics_df (pandas.core.frame.DataFrame)</p> <p>Dataframe of all model composed metrics</p> </li> <li> <p>dataset_name (str)</p> <p>Name of a dataset that was included in metric filenames and was used for the metrics computation</p> </li> <li> <p>model_names (list)</p> <p>Metrics for what model names to visualize</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these attributes</p> </li> </ul>"},{"location":"api/custom-classes/MetricsVisualizer/#methods","title":"Methods","text":"create_boxes_and_whiskers_for_models_multiple_runs <p>This boxes and whiskers plot is based on overall subgroup error and stability metrics for all defined models and results after all runs. Using it, you can see combined information on one plot that includes different models,  subgroup metrics, and results after multiple runs.</p> <p>Parameters</p> <ul> <li>metrics_lst     (list)    </li> </ul> create_fairness_variance_interactive_bar_chart <p>This interactive bar chart includes all groups, all composed group fairness and stability metrics,  and all defined models. Using it, you can select any pair of group fairness and stability metrics and   compare them across all groups and models. Since this plot is interactive, it saves a lot of space for other plots.    Also, it could be more convenient to compare each group fairness and stability metric using the interactive mode.</p> create_html_report <p>Create Fairness and Stability Report depending on report type. It includes visualizations and helpful details to them.</p> <p>Parameters</p> <ul> <li>report_type     (virny.configs.constants.ReportType)    </li> <li>report_save_path     (str)    </li> </ul> create_model_rank_heatmap <p>This heatmap includes all group fairness and stability metrics and all defined models. Using it, you can visually compare all models across all group metrics. On this plot, colors display ranks where 1 is the best model for the metric. These ranks are conditioned on difference or ratio operations used to create these group metrics:</p> <p>1) if the metric is created based on the difference operation, closer values to zero have ranks that are closer to the first rank  2) if the metric is created based on the ratio operation, closer values to one have ranks that are closer to the first rank</p> <p>Parameters</p> <ul> <li>model_metrics_matrix </li> <li>sorted_matrix_by_rank </li> <li>num_models     (int)    </li> </ul> create_model_rank_heatmaps <p>Create model rank and total model rank heatmaps.</p> <p>Parameters</p> <ul> <li>metrics_lst     (list)    </li> <li>groups_lst </li> </ul> create_models_metrics_bar_chart create_overall_metrics_bar_char <p>This bar chart includes all defined models and all overall subgroup error and stability metrics, which are averaged across multiple runs. Using it, you can compare all models for each subgroup error or stability metric. This comparison also includes reversed metrics, in which values closer to zero are better since straight and reversed metrics in this plot are converted to the same format -- values closer to one are better.</p> <p>Parameters</p> <ul> <li>metrics_names     (list)    </li> <li>reversed_metrics_names     (list)     \u2013 defaults to <code>None</code> </li> <li>metrics_title     (str)     \u2013 defaults to <code>Overall Metrics</code> </li> </ul> create_total_model_rank_heatmap <p>This heatmap includes all defined models and sums of their fairness and stability ranks. On this plot, colors display sums of ranks for one model. If the sum is smaller, the model has better fairness or stability characteristics than other models. Using this plot, you can visually compare all models for fairness and stability characteristics.</p> <p>Parameters</p> <ul> <li>sorted_matrix_by_rank </li> <li>num_models </li> </ul>"},{"location":"api/datasets/ACSEmploymentDataset/","title":"ACSEmploymentDataset","text":"<p>Dataset class for the employment task from the folktables dataset. Target: binary classification, predict if a person is employed. Source of the dataset: https://github.com/socialfoundations/folktables</p>"},{"location":"api/datasets/ACSEmploymentDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>state</p> <p>State in the US for which to get the data. All states in the US are available.</p> </li> <li> <p>year</p> <p>Year for which to get the data. Five different years of data collection are available: 2014\u20132018 inclusive.</p> </li> <li> <p>root_dir \u2013 defaults to <code>None</code></p> <p>Path to the root directory where to store the extracted dataset or where it is stored.</p> </li> <li> <p>with_nulls \u2013 defaults to <code>False</code></p> <p>Whether to keep nulls in the dataset or replace them on the new categorical class. Default: False.</p> </li> <li> <p>with_filter \u2013 defaults to <code>True</code></p> <p>Whether to use a folktables filter for this task. Default: True.</p> </li> <li> <p>optimize \u2013 defaults to <code>True</code></p> <p>Whether to optimize the dataset size by downcasting categorical columns. Default: True.</p> </li> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset.</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas.</p> </li> </ul>"},{"location":"api/datasets/ACSEmploymentDataset/#methods","title":"Methods","text":"update_X_data <p>To save simulated nulls</p> <p>Parameters</p> <ul> <li>X_data </li> </ul>"},{"location":"api/datasets/ACSIncomeDataset/","title":"ACSIncomeDataset","text":"<p>Dataset class for the income task from the folktables dataset. Target: binary classification, predict if a person has an annual income &gt; $50,000. Source of the dataset: https://github.com/socialfoundations/folktables</p>"},{"location":"api/datasets/ACSIncomeDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>state</p> <p>State in the US for which to get the data. All states in the US are available.</p> </li> <li> <p>year</p> <p>Year for which to get the data. Five different years of data collection are available: 2014\u20132018 inclusive.</p> </li> <li> <p>root_dir \u2013 defaults to <code>None</code></p> <p>Path to the root directory where to store the extracted dataset or where it is stored.</p> </li> <li> <p>with_nulls \u2013 defaults to <code>False</code></p> <p>Whether to keep nulls in the dataset or replace them on the new categorical class. Default: False.</p> </li> <li> <p>with_filter \u2013 defaults to <code>True</code></p> <p>Whether to use a folktables filter for this task. Default: True.</p> </li> <li> <p>optimize \u2013 defaults to <code>True</code></p> <p>Whether to optimize the dataset size by downcasting categorical columns. Default: True.</p> </li> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset.</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas.</p> </li> </ul>"},{"location":"api/datasets/ACSIncomeDataset/#methods","title":"Methods","text":"update_X_data <p>To save simulated nulls</p> <p>Parameters</p> <ul> <li>X_data </li> </ul>"},{"location":"api/datasets/ACSMobilityDataset/","title":"ACSMobilityDataset","text":"<p>Dataset class for the mobility task from the folktables dataset. Target: binary classification, predict whether a young adult moved addresses in the last year. Source of the dataset: https://github.com/socialfoundations/folktables</p>"},{"location":"api/datasets/ACSMobilityDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>state</p> <p>State in the US for which to get the data. All states in the US are available.</p> </li> <li> <p>year</p> <p>Year for which to get the data. Five different years of data collection are available: 2014\u20132018 inclusive.</p> </li> <li> <p>root_dir \u2013 defaults to <code>None</code></p> <p>Path to the root directory where to store the extracted dataset or where it is stored.</p> </li> <li> <p>with_nulls \u2013 defaults to <code>False</code></p> <p>Whether to keep nulls in the dataset or replace them on the new categorical class. Default: False.</p> </li> </ul>"},{"location":"api/datasets/ACSMobilityDataset/#methods","title":"Methods","text":"update_X_data <p>To save simulated nulls</p> <p>Parameters</p> <ul> <li>X_data </li> </ul>"},{"location":"api/datasets/ACSPublicCoverageDataset/","title":"ACSPublicCoverageDataset","text":"<p>Dataset class for the public coverage task from the folktables dataset. Target: binary classification, predict whether a low-income individual, not eligible for Medicare,     has coverage from public health insurance. Source of the dataset: https://github.com/socialfoundations/folktables</p>"},{"location":"api/datasets/ACSPublicCoverageDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>state</p> <p>State in the US for which to get the data. All states in the US are available.</p> </li> <li> <p>year</p> <p>Year for which to get the data. Five different years of data collection are available: 2014\u20132018 inclusive.</p> </li> <li> <p>root_dir \u2013 defaults to <code>None</code></p> <p>Path to the root directory where to store the extracted dataset or where it is stored.</p> </li> <li> <p>with_nulls \u2013 defaults to <code>False</code></p> <p>Whether to keep nulls in the dataset or replace them on the new categorical class. Default: False.</p> </li> <li> <p>with_filter \u2013 defaults to <code>True</code></p> <p>Whether to use a folktables filter for this task. Default: True.</p> </li> <li> <p>optimize \u2013 defaults to <code>True</code></p> <p>Whether to optimize the dataset size by downcasting categorical columns. Default: True.</p> </li> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset.</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas.</p> </li> </ul>"},{"location":"api/datasets/ACSPublicCoverageDataset/#methods","title":"Methods","text":"update_X_data <p>To save simulated nulls</p> <p>Parameters</p> <ul> <li>X_data </li> </ul>"},{"location":"api/datasets/ACSTravelTimeDataset/","title":"ACSTravelTimeDataset","text":"<p>Dataset class for the travel time task from the folktables dataset. Target: binary classification, predict whether a working adult has a travel time to work of greater than 20 minutes. Source of the dataset: https://github.com/socialfoundations/folktables</p>"},{"location":"api/datasets/ACSTravelTimeDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>state</p> <p>State in the US for which to get the data. All states in the US are available.</p> </li> <li> <p>year</p> <p>Year for which to get the data. Five different years of data collection are available: 2014\u20132018 inclusive.</p> </li> <li> <p>root_dir \u2013 defaults to <code>None</code></p> <p>Path to the root directory where to store the extracted dataset or where it is stored.</p> </li> <li> <p>with_nulls \u2013 defaults to <code>False</code></p> <p>Whether to keep nulls in the dataset or replace them on the new categorical class. Default: False.</p> </li> </ul>"},{"location":"api/datasets/ACSTravelTimeDataset/#methods","title":"Methods","text":"update_X_data <p>To save simulated nulls</p> <p>Parameters</p> <ul> <li>X_data </li> </ul>"},{"location":"api/datasets/CompasDataset/","title":"CompasDataset","text":"<p>Dataset class for the COMPAS dataset that contains sensitive attributes among feature columns.</p>"},{"location":"api/datasets/CompasDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> <li> <p>dataset_path \u2013 defaults to <code>None</code></p> <p>[Optional] Path to a file with the data</p> </li> </ul>"},{"location":"api/datasets/CompasWithoutSensitiveAttrsDataset/","title":"CompasWithoutSensitiveAttrsDataset","text":"<p>Dataset class for the COMPAS dataset that does not contain sensitive attributes among feature columns  to test blind classifiers</p>"},{"location":"api/datasets/CompasWithoutSensitiveAttrsDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> <li> <p>dataset_path \u2013 defaults to <code>None</code></p> <p>[Optional] Path to a file with the data</p> </li> </ul>"},{"location":"api/datasets/DiabetesDataset/","title":"DiabetesDataset","text":"<p>Dataset class for the Diabetes dataset that contains sensitive attributes among feature columns. Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/diabetes-clean.csv Description: https://arxiv.org/pdf/2110.00530.pdf</p>"},{"location":"api/datasets/DiabetesDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> <li> <p>dataset_path \u2013 defaults to <code>None</code></p> <p>[Optional] Path to a file with the data</p> </li> </ul>"},{"location":"api/datasets/LawSchoolDataset/","title":"LawSchoolDataset","text":"<p>Dataset class for the Law School dataset that contains sensitive attributes among feature columns. Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/law_school_clean.csv Description: https://arxiv.org/pdf/2110.00530.pdf</p>"},{"location":"api/datasets/LawSchoolDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> <li> <p>dataset_path \u2013 defaults to <code>None</code></p> <p>[Optional] Path to a file with the data</p> </li> </ul>"},{"location":"api/datasets/RicciDataset/","title":"RicciDataset","text":"<p>Dataset class for the Ricci dataset that contains sensitive attributes among feature columns. Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/ricci_race.csv Description: https://arxiv.org/pdf/2110.00530.pdf</p>"},{"location":"api/datasets/RicciDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset_path \u2013 defaults to <code>None</code></p> <p>[Optional] Path to a file with the data</p> </li> </ul>"},{"location":"api/metrics/compute-churn/","title":"compute_churn","text":"<p>Pairwise stability metric for two model predictions.</p>"},{"location":"api/metrics/compute-churn/#parameters","title":"Parameters","text":"<ul> <li> <p>predicted_labels_1 (list)</p> </li> <li> <p>predicted_labels_2 (list)</p> </li> </ul>"},{"location":"api/metrics/compute-conf-interval/","title":"compute_conf_interval","text":"<p>Create 95% confidence interval for population mean weight.</p>"},{"location":"api/metrics/compute-conf-interval/#parameters","title":"Parameters","text":"<ul> <li>labels</li> </ul>"},{"location":"api/metrics/compute-entropy-from-predicted-probability/","title":"compute_entropy_from_predicted_probability","text":"<p>Compute entropy from predicted probability</p>"},{"location":"api/metrics/compute-entropy-from-predicted-probability/#parameters","title":"Parameters","text":"<ul> <li> <p>x</p> <p>Probability of 0 class</p> </li> </ul>"},{"location":"api/metrics/compute-jitter/","title":"compute_jitter","text":"<p>Jitter is a stability metric that shows how the base model predictions fluctuate. Values closer to 0 -- perfect stability, values closer to 1 -- extremely bad stability.</p>"},{"location":"api/metrics/compute-jitter/#parameters","title":"Parameters","text":"<ul> <li>models_prediction_labels</li> </ul>"},{"location":"api/metrics/compute-per-sample-accuracy/","title":"compute_per_sample_accuracy","text":"<p>Compute per-sample accuracy for each model predictions.</p> <p>Return per_sample_accuracy and label_stability (refer to https://www.osti.gov/servlets/purl/1527311)</p>"},{"location":"api/metrics/compute-per-sample-accuracy/#parameters","title":"Parameters","text":"<ul> <li> <p>y_test</p> <p>y test dataset</p> </li> <li> <p>results</p> <p><code>results</code> variable from count_prediction_stats()</p> </li> </ul>"},{"location":"api/metrics/compute-std-mean-iqr-metrics/","title":"compute_std_mean_iqr_metrics","text":"<p>Compute mean, standard deviation, and interquartile range metrics.</p>"},{"location":"api/metrics/compute-std-mean-iqr-metrics/#parameters","title":"Parameters","text":"<ul> <li>results (pandas.core.frame.DataFrame)</li> </ul>"},{"location":"api/preprocessing/get-dummies/","title":"get_dummies","text":"<p>Return a dataset made by one-hot encoding for categorical columns and concatenate with numerical columns.</p>"},{"location":"api/preprocessing/get-dummies/#parameters","title":"Parameters","text":"<ul> <li> <p>data (pandas.core.frame.DataFrame)</p> <p>Dataframe for one-hot encoding</p> </li> <li> <p>categorical_columns (list)</p> <p>List of categorical column names</p> </li> <li> <p>numerical_columns (list)</p> <p>List of numerical column names</p> </li> </ul>"},{"location":"api/preprocessing/make-features-dfs/","title":"make_features_dfs","text":"<p>Return preprocessed train and test feature dataframes after one-hot encoding and standard scaling.</p>"},{"location":"api/preprocessing/make-features-dfs/#parameters","title":"Parameters","text":"<ul> <li> <p>X_train (pandas.core.frame.DataFrame)</p> </li> <li> <p>X_test (pandas.core.frame.DataFrame)</p> </li> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> </li> </ul>"},{"location":"api/preprocessing/preprocess-dataset/","title":"preprocess_dataset","text":"<p>Preprocess an input dataset using sklearn ColumnTransformer. Split the dataset on train and test using test_set_fraction.  Create an instance of BaseFlowDataset.</p>"},{"location":"api/preprocessing/preprocess-dataset/#parameters","title":"Parameters","text":"<ul> <li> <p>data_loader (virny.datasets.base.BaseDataLoader)</p> <p>Instance of BaseDataLoader that contains a target, numerical, and categorical columns.</p> </li> <li> <p>column_transformer (sklearn.compose._column_transformer.ColumnTransformer)</p> <p>Instance of sklearn ColumnTransformer to preprocess categorical and numerical columns.</p> </li> <li> <p>test_set_fraction (float)</p> <p>Fraction from 0 to 1. Used to split the input dataset on the train and test sets.</p> </li> <li> <p>dataset_split_seed (int)</p> <p>Seed for dataset splitting.</p> </li> </ul>"},{"location":"api/user-interfaces/compute-metrics-multiple-runs-with-db-writer/","title":"compute_metrics_multiple_runs_with_db_writer","text":"<p>Compute stability and accuracy metrics for each model in models_config. Arguments are defined as an input config object. Save results to a database after each run appending fields and value from custom_tbl_fields_dct and using db_writer_func.</p> <p>Return a dictionary where keys are model names, and values are metrics for sensitive attributes defined in config.</p>"},{"location":"api/user-interfaces/compute-metrics-multiple-runs-with-db-writer/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>BaseFlowDataset object that contains all needed attributes like target, features, numerical_columns etc.</p> </li> <li> <p>config</p> <p>Object that contains bootstrap_fraction, dataset_name, n_estimators, sensitive_attributes_dct attributes</p> </li> <li> <p>models_config (dict)</p> <p>Dictionary where keys are model names, and values are initialized models</p> </li> <li> <p>custom_tbl_fields_dct (dict)</p> <p>Dictionary where keys are column names and values to add to inserted metrics during saving results to a database</p> </li> <li> <p>db_writer_func</p> <p>Python function object has one argument (run_models_metrics_df) and save this metrics df to a target database</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.     As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/user-interfaces/compute-metrics-multiple-runs-with-multiple-test-sets/","title":"compute_metrics_multiple_runs_with_multiple_test_sets","text":"<p>Compute stability and accuracy metrics for each model in models_config based on dataset.X_test and each extra test set  in extra_test_sets_lst. Arguments are defined as an input config object. Save results to a database after each run   appending fields and value from custom_tbl_fields_dct and using db_writer_func.   Index of each test set is also added as a separate column in out final records in the database   (0 index -- for dataset.X_test, 1 and greater -- for each extra test set in extra_test_sets_lst, keeping the original sequence).</p>"},{"location":"api/user-interfaces/compute-metrics-multiple-runs-with-multiple-test-sets/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>BaseFlowDataset object that contains all needed attributes like target, features, numerical_columns etc.</p> </li> <li> <p>extra_test_sets_lst</p> <p>List of extra test sets like [(X_test1, y_test1), (X_test2, y_test2), ...] to compute metrics that are not equal to original dataset.X_test and dataset.y_test</p> </li> <li> <p>config</p> <p>Object that contains bootstrap_fraction, dataset_name, n_estimators, sensitive_attributes_dct attributes</p> </li> <li> <p>models_config (dict)</p> <p>Dictionary where keys are model names, and values are initialized models</p> </li> <li> <p>custom_tbl_fields_dct (dict)</p> <p>Dictionary where keys are column names and values to add to inserted metrics during saving results to a database</p> </li> <li> <p>db_writer_func</p> <p>Python function object has one argument (run_models_metrics_df) and save this metrics df to a target database</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.     As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/user-interfaces/compute-metrics-with-config/","title":"compute_metrics_with_config","text":"<p>Compute stability and accuracy metrics for each model in models_config. Arguments are defined as an input config object. Save results in <code>save_results_dir_path</code> folder.</p> <p>Return a dictionary where keys are model names, and values are metrics for sensitive attributes defined in config.</p>"},{"location":"api/user-interfaces/compute-metrics-with-config/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>BaseFlowDataset object that contains all needed attributes like target, features, numerical_columns etc.</p> </li> <li> <p>config</p> <p>Object that contains bootstrap_fraction, dataset_name, n_estimators, sensitive_attributes_dct attributes</p> </li> <li> <p>models_config (dict)</p> <p>Dictionary where keys are model names, and values are initialized models</p> </li> <li> <p>save_results_dir_path (str)</p> <p>Location where to save result files with metrics</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.     As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/user-interfaces/compute-model-metrics-with-config/","title":"compute_model_metrics_with_config","text":"<p>Compute subgroup metrics for the base model. Arguments are defined as an input config object. Save results in <code>save_results_dir_path</code> folder.</p> <p>Return a dataframe of model metrics.</p>"},{"location":"api/user-interfaces/compute-model-metrics-with-config/#parameters","title":"Parameters","text":"<ul> <li> <p>base_model</p> <p>Base model for metrics computation</p> </li> <li> <p>model_name (str)</p> <p>Model name to name a result file with metrics</p> </li> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>BaseFlowDataset object that contains all needed attributes like target, features, numerical_columns etc.</p> </li> <li> <p>config</p> <p>Object that contains bootstrap_fraction, dataset_name, n_estimators, sensitive_attributes_dct attributes</p> </li> <li> <p>save_results_dir_path (str)</p> <p>Location where to save result files with metrics</p> </li> <li> <p>save_results (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] If to save result metrics in a file</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.     As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/user-interfaces/compute-model-metrics/","title":"compute_model_metrics","text":"<p>Compute subgroup metrics for the base model. Save results in <code>save_results_dir_path</code> folder.</p> <p>Return a dataframe of model metrics.</p>"},{"location":"api/user-interfaces/compute-model-metrics/#parameters","title":"Parameters","text":"<ul> <li> <p>base_model</p> <p>Base model for metrics computation</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators for bootstrap to compute subgroup variance metrics</p> </li> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>BaseFlowDataset object that contains all needed attributes like target, features, numerical_columns etc.</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>Fraction of a train set in range [0.0 - 1.0] to fit models in bootstrap</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attribute names (including attributes intersections),  and values are privilege values for these attributes</p> </li> <li> <p>dataset_name (str)</p> <p>Dataset name to name a result file with metrics</p> </li> <li> <p>base_model_name (str)</p> <p>Model name to name a result file with metrics</p> </li> <li> <p>model_setting (str) \u2013 defaults to <code>batch</code></p> <p>[Optional] Model type: 'batch' or 'incremental'. Default: 'batch'.</p> </li> <li> <p>computation_mode (str) \u2013 defaults to <code>None</code></p> <p>[Optional] A non-default mode for metrics computation. Should be included in the ComputationMode enum.</p> </li> <li> <p>save_results (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] If to save result metrics in a file</p> </li> <li> <p>save_results_dir_path (str) \u2013 defaults to <code>None</code></p> <p>[Optional] Location where to save result files with metrics</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.     As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/user-interfaces/run-metrics-computation/","title":"run_metrics_computation","text":"<p>Compute stability and accuracy metrics for each model in models_config. Save results in <code>save_results_dir_path</code> folder.</p> <p>Return a dictionary where keys are model names, and values are metrics for sensitive attributes defined in config.</p>"},{"location":"api/user-interfaces/run-metrics-computation/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>Dataset object that contains all needed attributes like target, features, numerical_columns etc.</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>Fraction of a train set in range [0.0 - 1.0] to fit models in bootstrap</p> </li> <li> <p>dataset_name (str)</p> <p>Dataset name to name a result file with metrics</p> </li> <li> <p>models_config (dict)</p> <p>Dictionary where keys are model names, and values are initialized models</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators for bootstrap to compute subgroup stability metrics</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attribute names (including attributes intersections),  and values are privilege values for these attributes</p> </li> <li> <p>model_setting (str) \u2013 defaults to <code>batch</code></p> <p>[Optional] Model type: 'batch' or incremental. Default: 'batch'.</p> </li> <li> <p>computation_mode (str) \u2013 defaults to <code>None</code></p> <p>[Optional] A non-default mode for metrics computation. Should be included in the ComputationMode enum.</p> </li> <li> <p>save_results (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] If to save result metrics in a file</p> </li> <li> <p>save_results_dir_path (str) \u2013 defaults to <code>None</code></p> <p>[Optional] Location where to save result files with metrics</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.     As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/utils/count-prediction-stats/","title":"count_prediction_stats","text":"<p>Compute means, stds, iqr, entropy, jitter, label stability, and transform predictions to pd.Dataframe.</p> <p>Return a 1D numpy array of predictions, 2D array of each model prediction for y_test, a data structure of metrics.</p>"},{"location":"api/utils/count-prediction-stats/#parameters","title":"Parameters","text":"<ul> <li> <p>y_test</p> <p>True labels</p> </li> <li> <p>uq_results</p> <p>2D array of prediction proba for the zero value label by each model</p> </li> </ul>"},{"location":"api/utils/create-test-protected-groups/","title":"create_test_protected_groups","text":"<p>Create protected groups based on a test feature set. Use a disadvantaged group as a reference group.</p> <p>Return a dictionary where keys are subgroup names, and values are X_test row indexes correspondent to this subgroup.</p>"},{"location":"api/utils/create-test-protected-groups/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Test feature set</p> </li> <li> <p>init_features_df (pandas.core.frame.DataFrame)</p> <p>Initial full dataset without preprocessing</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attribute names (including attributes intersections),  and values are disadvantaged values for these attributes</p> </li> </ul>"},{"location":"api/utils/validate-config/","title":"validate_config","text":"<p>Validate parameters types and values in config yaml file.</p> <p>Extra details: * config_obj.model_setting is an optional argument that defines a type of models to use   to compute fairness and stability metrics. Should be 'batch' or 'incremental'. Default: 'batch'. </p> <ul> <li>config_obj.computation_mode is an optional argument that defines a non-default mode for metrics computation.   Currently, only 'error_analysis' mode is supported.</li> </ul>"},{"location":"api/utils/validate-config/#parameters","title":"Parameters","text":"<ul> <li> <p>config_obj</p> <p>Object with parameters defined in a yaml file</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_Use_Case/","title":"Multiple Models Interface Usage","text":"<p>In this example, we are going to audit 4 models for stability and fairness, visualize metrics, and create an analysis report. For that, we will use <code>compute_metrics_with_config</code> interface that can compute metrics for multiple models. Thus, we will need to do the next steps:</p> <ul> <li> <p>Initialize input variables</p> </li> <li> <p>Compute subgroup metrics</p> </li> <li> <p>Make group metrics composition</p> </li> <li> <p>Create metrics visualizations and an analysis report</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nimport pandas as pd\nfrom pprint import pprint\nfrom datetime import datetime, timezone\n\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom virny.utils.custom_initializers import create_config_obj, read_model_metric_dfs, create_models_config_from_tuned_params_df\nfrom virny.user_interfaces.metrics_computation_interfaces import compute_metrics_with_config\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.custom_classes.metrics_visualizer import MetricsVisualizer\nfrom virny.custom_classes.metrics_composer import MetricsComposer\nfrom virny.utils.model_tuning_utils import tune_ML_models\nfrom virny.datasets.base import BaseDataLoader\nfrom virny.configs.constants import ReportType\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metrics computation.</p> </li> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits for different analysis modes and analyze different types of models.</p> </li> </ul> <pre><code>DATASET_SPLIT_SEED = 42\nMODELS_TUNING_SEED = 42\nTEST_SET_FRACTION = 0.2\n</code></pre> <pre><code>models_params_for_tuning = {\n    'DecisionTreeClassifier': {\n        'model': DecisionTreeClassifier(random_state=MODELS_TUNING_SEED),\n        'params': {\n            \"max_depth\": [20, 30],\n            \"min_samples_split\" : [0.1],\n            \"max_features\": ['sqrt'],\n            \"criterion\": [\"gini\", \"entropy\"]\n        }\n    },\n    'LogisticRegression': {\n        'model': LogisticRegression(random_state=MODELS_TUNING_SEED),\n        'params': {\n            'penalty': ['l2'],\n            'C' : [0.0001, 0.1, 1, 100],\n            'solver': ['newton-cg', 'lbfgs'],\n            'max_iter': [250],\n        }\n    },\n    'RandomForestClassifier': {\n        'model': RandomForestClassifier(random_state=MODELS_TUNING_SEED),\n        'params': {\n            \"max_depth\": [6, 10],\n            \"min_samples_leaf\": [1],\n            \"n_estimators\": [50, 100],\n            \"max_features\": [0.6]\n        }\n    },\n    'XGBClassifier': {\n        'model': XGBClassifier(random_state=MODELS_TUNING_SEED, verbosity=0),\n        'params': {\n            'learning_rate': [0.1],\n            'n_estimators': [200],\n            'max_depth': [5, 7],\n            'lambda':  [10, 100]\n        }\n    }\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_with_config</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup stability metrics.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including attribute intersections), and values are privileged values for these attributes. Currently, the library supports only intersections among two sensitive attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify privileged values for intersectional groups since they will be derived from privileged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> </ul> <pre><code>ROOT_DIR = os.path.join('docs', 'examples')\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \"\"\"\ndataset_name: COMPAS_Without_Sensitive_Attributes\nbootstrap_fraction: 0.8\nn_estimators: 50  # Better to input the higher number of estimators than 100; this is only for this use case example\nsensitive_attributes_dct: {'sex': 1, 'race': 'African-American', 'sex&amp;race': None}\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\nSAVE_RESULTS_DIR_PATH = os.path.join(ROOT_DIR, 'results', f'{config.dataset_name}_Metrics_{datetime.now(timezone.utc).strftime(\"%Y%m%d__%H%M%S\")}')\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#preprocess-the-dataset-and-create-a-baseflowdataset-class","title":"Preprocess the dataset and create a BaseFlowDataset class","text":"<p>Based on the BaseDataset class, your dataset class should include the following attributes:</p> <ul> <li> <p>Obligatory attributes: dataset, target, features, numerical_columns, categorical_columns</p> </li> <li> <p>Optional attributes: X_data, y_data, columns_with_nulls</p> </li> </ul> <p>For more details, please refer to the library documentation.</p> <pre><code>class CompasWithoutSensitiveAttrsDataset(BaseDataLoader):\n\"\"\"\n    Dataset class for COMPAS dataset that does not contain sensitive attributes among feature columns\n     to test blind classifiers\n\n    Parameters\n    ----------\n    subsample_size\n        Subsample size to create based on the input dataset\n\n    \"\"\"\n    def __init__(self, dataset_path, subsample_size: int = None):\n        df = pd.read_csv(dataset_path)\n        if subsample_size:\n            df = df.sample(subsample_size)\n\n        # Initial data types transformation\n        int_columns = ['recidivism', 'age', 'age_cat_25 - 45', 'age_cat_Greater than 45',\n                       'age_cat_Less than 25', 'c_charge_degree_F', 'c_charge_degree_M', 'sex']\n        int_columns_dct = {col: \"int\" for col in int_columns}\n        df = df.astype(int_columns_dct)\n\n        # Define params\n        target = 'recidivism'\n        numerical_columns = ['juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count']\n        categorical_columns = ['age_cat_25 - 45', 'age_cat_Greater than 45','age_cat_Less than 25',\n                               'c_charge_degree_F', 'c_charge_degree_M']\n\n        super().__init__(\n            full_df=df,\n            target=target,\n            numerical_columns=numerical_columns,\n            categorical_columns=categorical_columns\n        )\n</code></pre> <pre><code>data_loader = CompasWithoutSensitiveAttrsDataset(dataset_path=os.path.join('virny', 'datasets', 'COMPAS.csv'))\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> juv_fel_count juv_misd_count juv_other_count priors_count age_cat_25 - 45 0 0.0 -2.340451 1.0 -15.010999 1 1 0.0 0.000000 0.0 0.000000 1 2 0.0 0.000000 0.0 0.000000 0 3 0.0 0.000000 0.0 6.000000 1 4 0.0 0.000000 0.0 7.513697 1 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code>base_flow_dataset = preprocess_dataset(data_loader, column_transformer, TEST_SET_FRACTION, DATASET_SPLIT_SEED)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#tune-models-and-create-a-models-config-for-metrics-computation","title":"Tune models and create a models config for metrics computation","text":"<pre><code>tuned_params_df, models_config = tune_ML_models(models_params_for_tuning, base_flow_dataset, config.dataset_name, n_folds=3)\ntuned_params_df\n</code></pre> <pre><code>2023/08/13, 01:39:20: Tuning DecisionTreeClassifier...\nFitting 3 folds for each of 4 candidates, totalling 12 fits\n2023/08/13, 01:39:22: Tuning for DecisionTreeClassifier is finished [F1 score = 0.6429262328840039, Accuracy = 0.6442550505050505]\n\n2023/08/13, 01:39:22: Tuning LogisticRegression...\nFitting 3 folds for each of 8 candidates, totalling 24 fits\n2023/08/13, 01:39:22: Tuning for LogisticRegression is finished [F1 score = 0.6461022173486363, Accuracy = 0.6505681818181818]\n\n2023/08/13, 01:39:22: Tuning RandomForestClassifier...\nFitting 3 folds for each of 4 candidates, totalling 12 fits\n2023/08/13, 01:39:23: Tuning for RandomForestClassifier is finished [F1 score = 0.6480756802972086, Accuracy = 0.6518308080808081]\n\n2023/08/13, 01:39:23: Tuning XGBClassifier...\nFitting 3 folds for each of 4 candidates, totalling 12 fits\n2023/08/13, 01:39:27: Tuning for XGBClassifier is finished [F1 score = 0.6548814644409034, Accuracy = 0.6587752525252525]\n</code></pre> Dataset_Name Model_Name F1_Score Accuracy_Score Model_Best_Params 0 COMPAS_Without_Sensitive_Attributes DecisionTreeClassifier 0.642926 0.644255 {'criterion': 'gini', 'max_depth': 20, 'max_fe... 1 COMPAS_Without_Sensitive_Attributes LogisticRegression 0.646102 0.650568 {'C': 1, 'max_iter': 250, 'penalty': 'l2', 'so... 2 COMPAS_Without_Sensitive_Attributes RandomForestClassifier 0.648076 0.651831 {'max_depth': 10, 'max_features': 0.6, 'min_sa... 3 COMPAS_Without_Sensitive_Attributes XGBClassifier 0.654881 0.658775 {'lambda': 100, 'learning_rate': 0.1, 'max_dep... <pre><code>now = datetime.now(timezone.utc)\ndate_time_str = now.strftime(\"%Y%m%d__%H%M%S\")\ntuned_df_path = os.path.join(ROOT_DIR, 'results', 'models_tuning', f'tuning_results_{config.dataset_name}_{date_time_str}.csv')\ntuned_params_df.to_csv(tuned_df_path, sep=\",\", columns=tuned_params_df.columns, float_format=\"%.4f\", index=False)\n</code></pre> <p>Create models_config from the saved tuned_params_df for higher reliability</p> <pre><code>models_config = create_models_config_from_tuned_params_df(models_params_for_tuning, tuned_df_path)\npprint(models_config)\n</code></pre> <pre><code>{'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=20, max_features='sqrt', min_samples_split=0.1,\n                       random_state=42),\n 'LogisticRegression': LogisticRegression(C=1, max_iter=250, random_state=42, solver='newton-cg'),\n 'RandomForestClassifier': RandomForestClassifier(max_depth=10, max_features=0.6, random_state=42),\n 'XGBClassifier': XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, lambda=100, learning_rate=0.1,\n              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n              predictor=None, ...)}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#subgroup-metrics-computation","title":"Subgroup Metrics Computation","text":"<p>After the variables are input to a user interface, the interface uses subgroup analyzers to compute different sets of metrics for each privileged and disadvantaged subgroup. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metrics computation, their metrics are combined, returned in a matrix format, and stored in a file if defined.</p> <pre><code>metrics_dct = compute_metrics_with_config(base_flow_dataset, config, models_config, SAVE_RESULTS_DIR_PATH, verbose=1)\n</code></pre> <p>A lot of logs...</p> <p>Look at several columns in top rows of computed metrics</p> <pre><code>sample_model_metrics_df = metrics_dct[list(models_config.keys())[0]]\nsample_model_metrics_df[sample_model_metrics_df.columns[:6]].head(20)\n</code></pre> Metric overall sex_priv sex_dis race_priv race_dis 0 Mean 0.521754 0.577841 0.507749 0.588128 0.478952 1 Std 0.070126 0.070049 0.070145 0.070509 0.069879 2 IQR 0.088289 0.085124 0.089079 0.091726 0.086072 3 Aleatoric_Uncertainty 0.867014 0.871857 0.865805 0.859131 0.872098 4 Overall_Uncertainty 0.892403 0.898704 0.890829 0.886515 0.896200 5 Statistical_Bias 0.419045 0.414878 0.420086 0.414936 0.421695 6 Jitter 0.122460 0.126825 0.121370 0.122124 0.122677 7 Per_Sample_Accuracy 0.679205 0.690711 0.676331 0.684203 0.675981 8 Label_Stability 0.833258 0.823886 0.835598 0.827536 0.836947 9 TPR 0.656051 0.493333 0.686869 0.523810 0.716049 10 TNR 0.726496 0.801471 0.703786 0.779026 0.682390 11 PPV 0.658849 0.578125 0.671605 0.566176 0.696697 12 FNR 0.343949 0.506667 0.313131 0.476190 0.283951 13 FPR 0.273504 0.198529 0.296214 0.220974 0.317610 14 Accuracy 0.695076 0.691943 0.695858 0.688406 0.699377 15 F1 0.657447 0.532374 0.679151 0.544170 0.706240 16 Selection-Rate 0.444129 0.303318 0.479290 0.328502 0.518692 17 Positive-Rate 0.995754 0.853333 1.022727 0.925170 1.027778 18 Sample_Size 1056.000000 NaN NaN NaN NaN"},{"location":"examples/Multiple_Models_Interface_Use_Case/#group-metrics-composition","title":"Group Metrics Composition","text":"<p>Metrics Composer is responsible for this second stage of the model audit. Currently, it computes our custom group fairness and stability metrics, but extending it for new group metrics is very simple. We noticed that more and more group metrics have appeared during the last decade, but most of them are based on the same subgroup metrics. Hence, such a separation of subgroup and group metrics computation allows one to experiment with different combinations of subgroup metrics and avoid subgroup metrics recomputation for a new set of grouped metrics.</p> <pre><code>models_metrics_dct = read_model_metric_dfs(SAVE_RESULTS_DIR_PATH, model_names=list(models_config.keys()))\n</code></pre> <pre><code>metrics_composer = MetricsComposer(models_metrics_dct, config.sensitive_attributes_dct)\n</code></pre> <p>Compute composed metrics</p> <pre><code>models_composed_metrics_df = metrics_composer.compose_metrics()\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#metrics-visualization-and-reporting","title":"Metrics Visualization and Reporting","text":"<p>Metrics Visualizer provides metrics visualization and reporting functionality. It unifies different preprocessing methods for result metrics and creates various data formats required for visualizations. Hence, users can simply call methods of the Metrics Visualizer class and get custom plots for diverse metrics analysis. Additionally, these plots could be collected in an HTML report with comments for user convenience and future reference.</p> <pre><code>visualizer = MetricsVisualizer(models_metrics_dct, models_composed_metrics_df, config.dataset_name,\n                               model_names=list(models_config.keys()),\n                               sensitive_attributes_dct=config.sensitive_attributes_dct)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metrics_names=['TPR', 'PPV', 'Accuracy', 'F1', 'Selection-Rate', 'Positive-Rate'],\n    metrics_title=\"Error Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metrics_names=['Label_Stability'],\n    reversed_metrics_names=['Std', 'IQR', 'Jitter'],\n    metrics_title=\"Variance Metrics\"\n)\n</code></pre> <p>Below is an example of an interactive plot. It requires that you run the below cell in Jupyter in the browser or EDAs, which support JavaScript displaying.</p> <p>You can use this plot to compare any pair of group fairness and stability metrics for all models.</p> <pre><code>visualizer.create_fairness_variance_interactive_bar_chart()\n</code></pre> <pre><code>visualizer.create_model_rank_heatmaps(\n    metrics_lst=[\n        # Group fairness metrics\n        'Equalized_Odds_TPR',\n        'Equalized_Odds_FPR',\n        'Disparate_Impact',\n        'Statistical_Parity_Difference',\n        'Accuracy_Parity',\n        # Group stability metrics\n        'Label_Stability_Ratio',\n        'IQR_Parity',\n        'Std_Parity',\n        'Std_Ratio',\n        'Jitter_Parity',\n    ],\n    groups_lst=config.sensitive_attributes_dct.keys(),\n)\n</code></pre> <p></p> <p></p> <p>Create an analysis report. It includes correspondent visualizations and details about your result metrics.</p> <pre><code>visualizer.create_html_report(report_type=ReportType.MULTIPLE_RUNS_MULTIPLE_MODELS,\n                              report_save_path=os.path.join(ROOT_DIR, \"results\", \"reports\"))\n</code></pre> <p>App saved to ./docs/examples/results/reports/COMPAS_Without_Sensitive_Attributes_Metrics_Report_20230812__224023.html</p>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/","title":"Multiple Models Interface Usage With DB Writer","text":"<p>In this example, we are going to audit 4 models for stability and fairness, visualize metrics, and create an analysis report. To get better analysis accuracy, we will use <code>compute_metrics_multiple_runs_with_db_writer</code> interface that will compute metrics for multiple models and save results in the user database based on the db_writer function. For that, we will need to do the next steps:</p> <ul> <li> <p>Initialize input variables</p> </li> <li> <p>Compute subgroup metrics</p> </li> <li> <p>Make group metrics composition</p> </li> <li> <p>Create metrics visualizations and an analysis report</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nimport pandas as pd\n\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom virny.user_interfaces.metrics_computation_interfaces import compute_metrics_multiple_runs_with_db_writer\nfrom virny.utils.custom_initializers import create_config_obj, create_models_metrics_dct_from_database_df\nfrom virny.custom_classes.metrics_visualizer import MetricsVisualizer\nfrom virny.custom_classes.metrics_composer import MetricsComposer\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.datasets.data_loaders import CompasWithoutSensitiveAttrsDataset\nfrom virny.configs.constants import ReportType\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metrics computation.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits of multiple models and analyze different types of models.</p> </li> </ul> <pre><code>TEST_SET_FRACTION = 0.2\nDATASET_SPLIT_SEED = 42\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#create-a-dataset-class","title":"Create a Dataset class","text":"<p>Based on the BaseDataset class, your dataset class should include the following attributes:</p> <ul> <li> <p>Obligatory attributes: dataset, target, features, numerical_columns, categorical_columns</p> </li> <li> <p>Optional attributes: X_data, y_data, columns_with_nulls</p> </li> </ul> <p>For more details, please refer to the library documentation.</p> <pre><code>data_loader = CompasWithoutSensitiveAttrsDataset()\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> juv_fel_count juv_misd_count juv_other_count priors_count age_cat_25 - 45 0 0.0 -2.340451 1.0 -15.010999 1 1 0.0 0.000000 0.0 0.000000 1 2 0.0 0.000000 0.0 0.000000 0 3 0.0 0.000000 0.0 6.000000 1 4 0.0 0.000000 0.0 7.513697 1 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code>base_flow_dataset = preprocess_dataset(data_loader, column_transformer, TEST_SET_FRACTION, DATASET_SPLIT_SEED)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_multiple_runs_with_db_writer</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup stability metrics.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including attribute intersections), and values are privileged values for these attributes. Currently, the library supports only intersections among two sensitive attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify privileged values for intersectional groups since they will be derived from privileged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> </ul> <pre><code>ROOT_DIR = os.getcwd()\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \\\n\"\"\"dataset_name: COMPAS_Without_Sensitive_Attributes\nbootstrap_fraction: 0.8\nn_estimators: 50  # Better to input the higher number of estimators than 100; this is only for this use case example\nsensitive_attributes_dct: {'sex': 1, 'race': 'African-American', 'sex&amp;race': None}\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#create-a-models-config","title":"Create a models config","text":"<p>models_config is a Python dictionary, where keys are model names and values are initialized models for analysis</p> <pre><code>models_config = {\n    'DecisionTreeClassifier': DecisionTreeClassifier(criterion='gini',\n                                                     max_depth=20,\n                                                     max_features=0.6,\n                                                     min_samples_split=0.1),\n    'LogisticRegression': LogisticRegression(C=1,\n                                             max_iter=50,\n                                             penalty='l2',\n                                             solver='newton-cg'),\n    'RandomForestClassifier': RandomForestClassifier(max_depth=4,\n                                                     max_features=0.6,\n                                                     min_samples_leaf=1,\n                                                     n_estimators=50),\n    'XGBClassifier': XGBClassifier(learning_rate=0.1,\n                                   max_depth=5,\n                                   n_estimators=20),\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#subgroup-metrics-computation","title":"Subgroup Metrics Computation","text":"<p>After the variables are input to a user interface, the interface uses subgroup analyzers to compute different sets of metrics for each privileged and disprivileged subgroup. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metrics computation, their metrics are combined, returned in a matrix format, and stored in a file if defined.</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom pymongo import MongoClient\n\n\nload_dotenv(os.path.join(ROOT_DIR, 'secrets.env'))  # Take environment variables from .env\n\n# Provide the mongodb atlas url to connect python to mongodb using pymongo\nCONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n# Create a connection using MongoClient. You can import MongoClient or use pymongo.MongoClient\nclient = MongoClient(CONNECTION_STRING)\ncollection = client[os.getenv(\"DB_NAME\")]['preprocessing_results']\n\n\ndef db_writer_func(run_models_metrics_df, collection=collection):\n    run_models_metrics_df.columns = run_models_metrics_df.columns.str.lower()  # Rename Pandas columns to lower case\n    collection.insert_many(run_models_metrics_df.to_dict('records'))\n</code></pre> <pre><code>import uuid\n\ncustom_table_fields_dct = {\n    'session_uuid': str(uuid.uuid4()),\n    'preprocessing_techniques': 'get_dummies and scaler',\n}\nprint('Current session uuid: ', custom_table_fields_dct['session_uuid'])\n</code></pre> <pre><code>Current session uuid:  7013282e-2bf0-42df-ac7a-acf338c1cf58\n</code></pre> <p><pre><code>metrics_dct = compute_metrics_multiple_runs_with_db_writer(base_flow_dataset, config, models_config, custom_table_fields_dct,\n                                                           db_writer_func)\n</code></pre> A lot of logs...</p> <p>Look at several columns in top rows of computed metrics</p> <pre><code>sample_model_metrics_df = metrics_dct[list(models_config.keys())[0]]\nsample_model_metrics_df[sample_model_metrics_df.columns[:6]].head(20)\n</code></pre> Metric overall sex_priv sex_dis race_priv race_dis 0 Mean 0.520075 0.575481 0.506240 0.584422 0.478580 1 Std 0.073216 0.076031 0.072513 0.073452 0.073063 2 IQR 0.088307 0.087492 0.088510 0.088613 0.088109 3 Aleatoric_Uncertainty 0.861499 0.868571 0.859733 0.854025 0.866318 4 Overall_Uncertainty 0.888512 0.897406 0.886291 0.882379 0.892467 5 Statistical_Bias 0.416186 0.411654 0.417318 0.411442 0.419246 6 Jitter 0.113070 0.135874 0.107375 0.111362 0.114171 7 Per_Sample_Accuracy 0.687519 0.688720 0.687219 0.692222 0.684486 8 Label_Stability 0.858295 0.822938 0.867124 0.856715 0.859315 9 TPR 0.656051 0.466667 0.691919 0.517007 0.719136 10 TNR 0.733333 0.808824 0.710468 0.786517 0.688679 11 PPV 0.664516 0.573770 0.678218 0.571429 0.701807 12 FNR 0.343949 0.533333 0.308081 0.482993 0.280864 13 FPR 0.266667 0.191176 0.289532 0.213483 0.311321 14 Accuracy 0.698864 0.687204 0.701775 0.690821 0.704050 15 F1 0.660256 0.514706 0.685000 0.542857 0.710366 16 Selection-Rate 0.440341 0.289100 0.478107 0.321256 0.517134 17 Positive-Rate 0.987261 0.813333 1.020202 0.904762 1.024691 18 Sample_Size 1056.000000 NaN NaN NaN NaN"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#group-metrics-composition","title":"Group Metrics Composition","text":"<p>Metrics Composer is responsible for this second stage of the model audit. Currently, it computes our custom group fairness and stability metrics, but extending it for new group metrics is very simple. We noticed that more and more group metrics have appeared during the last decade, but most of them are based on the same subgroup metrics. Hence, such a separation of subgroup and group metrics computation allows one to experiment with different combinations of subgroup metrics and avoid subgroup metrics recomputation for a new set of grouped metrics.</p> <pre><code>def read_model_metric_dfs_from_db(collection, session_uuid):\n    cursor = collection.find({'session_uuid': session_uuid})\n    records = []\n    for record in cursor:\n        del record['_id']\n        records.append(record)\n\n    model_metric_dfs = pd.DataFrame(records)\n\n    # Capitalize column names to be consistent across the whole library\n    new_column_names = []\n    for col in model_metric_dfs.columns:\n        new_col_name = '_'.join([c.capitalize() for c in col.split('_')])\n        new_column_names.append(new_col_name)\n\n    model_metric_dfs.columns = new_column_names\n    return model_metric_dfs\n</code></pre> <pre><code>model_metric_dfs = read_model_metric_dfs_from_db(collection, custom_table_fields_dct['session_uuid'])\nmodels_metrics_dct = create_models_metrics_dct_from_database_df(model_metric_dfs)\n</code></pre> <pre><code>metrics_composer = MetricsComposer(models_metrics_dct, config.sensitive_attributes_dct)\n</code></pre> <p>Compute composed metrics</p> <pre><code>models_composed_metrics_df = metrics_composer.compose_metrics()\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#metrics-visualization-and-reporting","title":"Metrics Visualization and Reporting","text":"<p>Metrics Visualizer provides metrics visualization and reporting functionality. It unifies different preprocessing methods for result metrics and creates various data formats required for visualizations. Hence, users can simply call methods of the Metrics Visualizer class and get custom plots for diverse metrics analysis. Additionally, these plots could be collected in an HTML report with comments for user convenience and future reference.</p> <pre><code>visualizer = MetricsVisualizer(models_metrics_dct, models_composed_metrics_df, config.dataset_name,\n                               model_names=list(models_config.keys()),\n                               sensitive_attributes_dct=config.sensitive_attributes_dct)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metrics_names=['TPR', 'PPV', 'Accuracy', 'F1', 'Selection-Rate', 'Positive-Rate'],\n    metrics_title=\"Error Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metrics_names=['Label_Stability'],\n    reversed_metrics_names=['Std', 'IQR', 'Jitter'],\n    metrics_title=\"Variance Metrics\"\n)\n</code></pre> <p>Below is an example of an interactive plot. It requires that you run the below cell in Jupyter in the browser or EDAs, which support JavaScript displaying.</p> <p>You can use this plot to compare any pair of group fairness and stability metrics for all models.</p> <pre><code>visualizer.create_fairness_variance_interactive_bar_chart()\n</code></pre> <pre><code>visualizer.create_model_rank_heatmaps(\n    metrics_lst=[\n        # Group fairness metrics\n        'Equalized_Odds_TPR',\n        'Equalized_Odds_FPR',\n        'Disparate_Impact',\n        'Statistical_Parity_Difference',\n        'Accuracy_Parity',\n        # Group variance metrics\n        'Label_Stability_Ratio',\n        'IQR_Parity',\n        'Std_Parity',\n        'Std_Ratio',\n        'Jitter_Parity',\n    ],\n    groups_lst=config.sensitive_attributes_dct.keys(),\n)\n</code></pre> <p></p> <p></p> <p>Create an analysis report. It includes correspondent visualizations and details about your result metrics.</p> <pre><code>client.close()\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/","title":"Multiple Models Interface With Error Analysis","text":"<p>In this example, we are going to audit 4 models for stability and fairness, visualize metrics, and create an analysis report. The only difference with the multiple models interface tutorial is the use of an <code>error_analysis</code> computation mode. This mode measures subgroup and group metrics also for correct and incorrect predictions. For example, when a default computation mode measures metrics for sex_priv and sex_dis, an <code>error_analysis</code> mode measures metrics for (sex_priv, sex_priv_correct, sex_priv_incorrect) and (sex_dis, sex_dis_correct, sex_dis_incorrect). Therefore, a user can analyze how a model is certain about its incorrect predictions.</p> <p>For that, we will use <code>compute_metrics_with_config</code> interface that can compute metrics for multiple models. Thus, we will need to do the next steps:</p> <ul> <li> <p>Initialize input variables</p> </li> <li> <p>Compute subgroup metrics</p> </li> <li> <p>Make group metrics composition</p> </li> <li> <p>Create metrics visualizations and an analysis report</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nimport pandas as pd\nfrom pprint import pprint\nfrom datetime import datetime, timezone\n\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom virny.utils.custom_initializers import create_config_obj, read_model_metric_dfs, create_models_config_from_tuned_params_df\nfrom virny.user_interfaces.metrics_computation_interfaces import compute_metrics_with_config\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.custom_classes.metrics_visualizer import MetricsVisualizer\nfrom virny.custom_classes.metrics_composer import MetricsComposer\nfrom virny.utils.model_tuning_utils import tune_ML_models\nfrom virny.datasets.base import BaseDataLoader\nfrom virny.configs.constants import ReportType\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metrics computation.</p> </li> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits for different analysis modes and analyze different types of models.</p> </li> </ul> <pre><code>DATASET_SPLIT_SEED = 42\nMODELS_TUNING_SEED = 42\nTEST_SET_FRACTION = 0.2\n</code></pre> <pre><code>models_params_for_tuning = {\n    'DecisionTreeClassifier': {\n        'model': DecisionTreeClassifier(random_state=MODELS_TUNING_SEED),\n        'params': {\n            \"max_depth\": [20, 30],\n            \"min_samples_split\" : [0.1],\n            \"max_features\": ['sqrt'],\n            \"criterion\": [\"gini\", \"entropy\"]\n        }\n    },\n    'LogisticRegression': {\n        'model': LogisticRegression(random_state=MODELS_TUNING_SEED),\n        'params': {\n            'penalty': ['l2'],\n            'C' : [0.0001, 0.1, 1, 100],\n            'solver': ['newton-cg', 'lbfgs'],\n            'max_iter': [250],\n        }\n    },\n    'RandomForestClassifier': {\n        'model': RandomForestClassifier(random_state=MODELS_TUNING_SEED),\n        'params': {\n            \"max_depth\": [6, 10],\n            \"min_samples_leaf\": [1],\n            \"n_estimators\": [50, 100],\n            \"max_features\": [0.6]\n        }\n    },\n    'XGBClassifier': {\n        'model': XGBClassifier(random_state=MODELS_TUNING_SEED, verbosity=0),\n        'params': {\n            'learning_rate': [0.1],\n            'n_estimators': [200],\n            'max_depth': [5, 7],\n            'lambda':  [10, 100]\n        }\n    }\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_with_config</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup stability metrics.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including attribute intersections), and values are privileged values for these attributes. Currently, the library supports only intersections among two sensitive attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify privileged values for intersectional groups since they will be derived from privileged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> </ul> <pre><code>ROOT_DIR = os.path.join('docs', 'examples')\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \"\"\"\ndataset_name: COMPAS_Without_Sensitive_Attributes\nbootstrap_fraction: 0.8\nn_estimators: 50  # Better to input the higher number of estimators than 100; this is only for this use case example\ncomputation_mode: error_analysis\nsensitive_attributes_dct: {'sex': 1, 'race': 'African-American', 'sex&amp;race': None}\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\nSAVE_RESULTS_DIR_PATH = os.path.join(ROOT_DIR, 'results', f'{config.dataset_name}_Metrics_{datetime.now(timezone.utc).strftime(\"%Y%m%d__%H%M%S\")}')\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#preprocess-the-dataset-and-create-a-baseflowdataset-class","title":"Preprocess the dataset and create a BaseFlowDataset class","text":"<p>Based on the BaseDataset class, your dataset class should include the following attributes:</p> <ul> <li> <p>Obligatory attributes: dataset, target, features, numerical_columns, categorical_columns</p> </li> <li> <p>Optional attributes: X_data, y_data, columns_with_nulls</p> </li> </ul> <p>For more details, please refer to the library documentation.</p> <pre><code>class CompasWithoutSensitiveAttrsDataset(BaseDataLoader):\n\"\"\"\n    Dataset class for COMPAS dataset that does not contain sensitive attributes among feature columns\n     to test blind classifiers\n\n    Parameters\n    ----------\n    subsample_size\n        Subsample size to create based on the input dataset\n\n    \"\"\"\n    def __init__(self, dataset_path, subsample_size: int = None):\n        df = pd.read_csv(dataset_path)\n        if subsample_size:\n            df = df.sample(subsample_size)\n\n        # Initial data types transformation\n        int_columns = ['recidivism', 'age', 'age_cat_25 - 45', 'age_cat_Greater than 45',\n                       'age_cat_Less than 25', 'c_charge_degree_F', 'c_charge_degree_M', 'sex']\n        int_columns_dct = {col: \"int\" for col in int_columns}\n        df = df.astype(int_columns_dct)\n\n        # Define params\n        target = 'recidivism'\n        numerical_columns = ['juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count']\n        categorical_columns = ['age_cat_25 - 45', 'age_cat_Greater than 45','age_cat_Less than 25',\n                               'c_charge_degree_F', 'c_charge_degree_M']\n\n        super().__init__(\n            full_df=df,\n            target=target,\n            numerical_columns=numerical_columns,\n            categorical_columns=categorical_columns\n        )\n</code></pre> <pre><code>data_loader = CompasWithoutSensitiveAttrsDataset(dataset_path=os.path.join('virny', 'datasets', 'COMPAS.csv'))\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> juv_fel_count juv_misd_count juv_other_count priors_count age_cat_25 - 45 0 0.0 -2.340451 1.0 -15.010999 1 1 0.0 0.000000 0.0 0.000000 1 2 0.0 0.000000 0.0 0.000000 0 3 0.0 0.000000 0.0 6.000000 1 4 0.0 0.000000 0.0 7.513697 1 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code>base_flow_dataset = preprocess_dataset(data_loader, column_transformer, TEST_SET_FRACTION, DATASET_SPLIT_SEED)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#tune-models-and-create-a-models-config-for-metrics-computation","title":"Tune models and create a models config for metrics computation","text":"<pre><code>tuned_params_df, models_config = tune_ML_models(models_params_for_tuning, base_flow_dataset, config.dataset_name, n_folds=3)\ntuned_params_df\n</code></pre> <pre><code>2023/08/13, 01:41:37: Tuning DecisionTreeClassifier...\nFitting 3 folds for each of 4 candidates, totalling 12 fits\n2023/08/13, 01:41:38: Tuning for DecisionTreeClassifier is finished [F1 score = 0.6429262328840039, Accuracy = 0.6442550505050505]\n\n2023/08/13, 01:41:38: Tuning LogisticRegression...\nFitting 3 folds for each of 8 candidates, totalling 24 fits\n2023/08/13, 01:41:38: Tuning for LogisticRegression is finished [F1 score = 0.6461022173486363, Accuracy = 0.6505681818181818]\n\n2023/08/13, 01:41:38: Tuning RandomForestClassifier...\nFitting 3 folds for each of 4 candidates, totalling 12 fits\n2023/08/13, 01:41:39: Tuning for RandomForestClassifier is finished [F1 score = 0.6480756802972086, Accuracy = 0.6518308080808081]\n\n2023/08/13, 01:41:39: Tuning XGBClassifier...\nFitting 3 folds for each of 4 candidates, totalling 12 fits\n2023/08/13, 01:41:42: Tuning for XGBClassifier is finished [F1 score = 0.6548814644409034, Accuracy = 0.6587752525252525]\n</code></pre> Dataset_Name Model_Name F1_Score Accuracy_Score Model_Best_Params 0 COMPAS_Without_Sensitive_Attributes DecisionTreeClassifier 0.642926 0.644255 {'criterion': 'gini', 'max_depth': 20, 'max_fe... 1 COMPAS_Without_Sensitive_Attributes LogisticRegression 0.646102 0.650568 {'C': 1, 'max_iter': 250, 'penalty': 'l2', 'so... 2 COMPAS_Without_Sensitive_Attributes RandomForestClassifier 0.648076 0.651831 {'max_depth': 10, 'max_features': 0.6, 'min_sa... 3 COMPAS_Without_Sensitive_Attributes XGBClassifier 0.654881 0.658775 {'lambda': 100, 'learning_rate': 0.1, 'max_dep... <pre><code>now = datetime.now(timezone.utc)\ndate_time_str = now.strftime(\"%Y%m%d__%H%M%S\")\ntuned_df_path = os.path.join(ROOT_DIR, 'results', 'models_tuning', f'tuning_results_{config.dataset_name}_{date_time_str}.csv')\ntuned_params_df.to_csv(tuned_df_path, sep=\",\", columns=tuned_params_df.columns, float_format=\"%.4f\", index=False)\n</code></pre> <p>Create models_config from the saved tuned_params_df for higher reliability</p> <pre><code>models_config = create_models_config_from_tuned_params_df(models_params_for_tuning, tuned_df_path)\npprint(models_config)\n</code></pre> <pre><code>{'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=20, max_features='sqrt', min_samples_split=0.1,\n                       random_state=42),\n 'LogisticRegression': LogisticRegression(C=1, max_iter=250, random_state=42, solver='newton-cg'),\n 'RandomForestClassifier': RandomForestClassifier(max_depth=10, max_features=0.6, random_state=42),\n 'XGBClassifier': XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, lambda=100, learning_rate=0.1,\n              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n              predictor=None, ...)}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#subgroup-metrics-computation","title":"Subgroup Metrics Computation","text":"<p>After the variables are input to a user interface, the interface uses subgroup analyzers to compute different sets of metrics for each privileged and disprivileged subgroup. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metrics computation, their metrics are combined, returned in a matrix format, and stored in a file if defined.</p> <p><pre><code>metrics_dct = compute_metrics_with_config(base_flow_dataset, config, models_config, SAVE_RESULTS_DIR_PATH)\n</code></pre> A lot of logs...</p> <p>Look at several columns in top rows of computed metrics. Note that now we have metrics also for <code>*_correct</code> and <code>*_incorrect</code> subgroups.</p> <pre><code>sample_model_metrics_df = metrics_dct[list(models_config.keys())[0]]\nsample_model_metrics_df[sample_model_metrics_df.columns[:5]].head(20)\n</code></pre> Metric overall sex_priv sex_priv_correct sex_priv_incorrect 0 Mean 0.520371 0.575423 0.597496 0.525844 1 Std 0.070673 0.074759 0.071142 0.082884 2 IQR 0.084966 0.084351 0.081560 0.090620 3 Aleatoric_Uncertainty 0.868870 0.870161 0.853201 0.908254 4 Overall_Uncertainty 0.893937 0.898822 0.880148 0.940765 5 Statistical_Bias 0.419189 0.414832 0.326713 0.612761 6 Jitter 0.134273 0.145329 0.113939 0.215837 7 Per_Sample_Accuracy 0.678542 0.687393 0.923562 0.156923 8 Label_Stability 0.815644 0.799810 0.847671 0.692308 9 TPR 0.658174 0.480000 1.000000 0.000000 10 TNR 0.733333 0.808824 1.000000 0.000000 11 PPV 0.665236 0.580645 1.000000 0.000000 12 FNR 0.341826 0.520000 0.000000 1.000000 13 FPR 0.266667 0.191176 0.000000 1.000000 14 Accuracy 0.699811 0.691943 1.000000 0.000000 15 F1 0.661686 0.525547 1.000000 0.000000 16 Selection-Rate 0.441288 0.293839 0.246575 0.400000 17 Positive-Rate 0.989384 0.826667 1.000000 0.666667 18 Sample_Size 1056.000000 211.000000 146.000000 65.000000"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#group-metrics-composition","title":"Group Metrics Composition","text":"<p>Metrics Composer is responsible for this second stage of the model audit. Currently, it computes our custom group fairness and stability metrics, but extending it for new group metrics is very simple. We noticed that more and more group metrics have appeared during the last decade, but most of them are based on the same subgroup metrics. Hence, such a separation of subgroup and group metrics computation allows one to experiment with different combinations of subgroup metrics and avoid subgroup metrics recomputation for a new set of grouped metrics.</p> <pre><code>models_metrics_dct = read_model_metric_dfs(SAVE_RESULTS_DIR_PATH, model_names=list(models_config.keys()))\n</code></pre> <pre><code>metrics_composer = MetricsComposer(models_metrics_dct, config.sensitive_attributes_dct)\n</code></pre> <p>Compute composed metrics</p> <pre><code>models_composed_metrics_df = metrics_composer.compose_metrics()\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#metrics-visualization-and-reporting","title":"Metrics Visualization and Reporting","text":"<p>Metrics Visualizer provides metrics visualization and reporting functionality. It unifies different preprocessing methods for result metrics and creates various data formats required for visualizations. Hence, users can simply call methods of the Metrics Visualizer class and get custom plots for diverse metrics analysis. Additionally, these plots could be collected in an HTML report with comments for user convenience and future reference.</p> <pre><code>visualizer = MetricsVisualizer(models_metrics_dct, models_composed_metrics_df, config.dataset_name,\n                               model_names=list(models_config.keys()),\n                               sensitive_attributes_dct=config.sensitive_attributes_dct)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metrics_names=['TPR', 'PPV', 'Accuracy', 'F1', 'Selection-Rate', 'Positive-Rate'],\n    metrics_title=\"Error Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metrics_names=['Label_Stability', 'Aleatoric_Uncertainty', 'Overall_Uncertainty'],\n    reversed_metrics_names=['Std', 'IQR', 'Jitter'],\n    metrics_title=\"Variance Metrics\"\n)\n</code></pre> <p>Below is an example of an interactive plot. It requires that you run the below cell in Jupyter in the browser or EDAs, which support JavaScript displaying.</p> <p>You can use this plot to compare any pair of group fairness and stability metrics for all models.</p> <pre><code>visualizer.create_fairness_variance_interactive_bar_chart()\n</code></pre> <pre><code>visualizer.create_model_rank_heatmaps(\n    metrics_lst=[\n        # Group fairness metrics\n        'Equalized_Odds_TPR',\n        'Equalized_Odds_FPR',\n        'Disparate_Impact',\n        'Statistical_Parity_Difference',\n        'Accuracy_Parity',\n        # Group stability metrics\n        'Label_Stability_Ratio',\n        'IQR_Parity',\n        'Std_Parity',\n        'Std_Ratio',\n        'Jitter_Parity',\n    ],\n    groups_lst=config.sensitive_attributes_dct.keys(),\n)\n</code></pre> <p></p> <p></p> <p>Create an analysis report. It includes correspondent visualizations and details about your result metrics.</p> <pre><code>visualizer.create_html_report(report_type=ReportType.MULTIPLE_RUNS_MULTIPLE_MODELS,\n                              report_save_path=os.path.join(ROOT_DIR, \"results\", \"reports\"))\n</code></pre> <p>App saved to ./docs/examples/results/reports/COMPAS_Without_Sensitive_Attributes_Metrics_Report_20230812__224310.html</p>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/","title":"Multiple Models Interface For Multiple Test Sets","text":"<p>In this example, we are going to audit 2 models for stability and fairness, visualize metrics, and create an analysis report. To get better analysis accuracy, we will use <code>compute_metrics_multiple_runs_with_multiple_test_sets</code> interface that will run metric computation for multiple models and test each model using multiple test sets.</p>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom virny.user_interfaces.metrics_computation_interfaces import compute_metrics_multiple_runs_with_multiple_test_sets\nfrom virny.utils.custom_initializers import create_config_obj, create_models_metrics_dct_from_database_df\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.datasets.data_loaders import CompasWithoutSensitiveAttrsDataset\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metrics computation.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits of multiple models and analyze different types of models.</p> </li> </ul> <pre><code>TEST_SET_FRACTION = 0.2\nDATASET_SPLIT_SEED = 42\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#create-a-dataset-class","title":"Create a Dataset class","text":"<p>Based on the BaseDataset class, your dataset class should include the following attributes:</p> <ul> <li> <p>Obligatory attributes: dataset, target, features, numerical_columns, categorical_columns</p> </li> <li> <p>Optional attributes: X_data, y_data, columns_with_nulls</p> </li> </ul> <p>For more details, please refer to the library documentation.</p> <pre><code>data_loader = CompasWithoutSensitiveAttrsDataset()\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> juv_fel_count juv_misd_count juv_other_count priors_count age_cat_25 - 45 0 0.0 -2.340451 1.0 -15.010999 1 1 0.0 0.000000 0.0 0.000000 1 2 0.0 0.000000 0.0 0.000000 0 3 0.0 0.000000 0.0 6.000000 1 4 0.0 0.000000 0.0 7.513697 1 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code>base_flow_dataset = preprocess_dataset(data_loader, column_transformer, TEST_SET_FRACTION, DATASET_SPLIT_SEED)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_multiple_runs_with_multiple_test_sets</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup variance metrics.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including attribute intersections), and values are privileged values for these attributes. Currently, the library supports only intersections among two sensitive attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify privileged values for intersectional groups since they will be derived from privileged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> </ul> <pre><code>ROOT_DIR = os.getcwd()\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \\\n\"\"\"dataset_name: COMPAS_Without_Sensitive_Attributes\nbootstrap_fraction: 0.8\nn_estimators: 50  # Better to input the higher number of estimators than 100; this is only for this use case example\ncomputation_mode: error_analysis\nsensitive_attributes_dct: {'sex': 1, 'race': 'African-American', 'sex&amp;race': None}\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#create-a-models-config","title":"Create a models config","text":"<p>models_config is a Python dictionary, where keys are model names and values are initialized models for analysis</p> <pre><code>models_config = {\n    'DecisionTreeClassifier': DecisionTreeClassifier(criterion='gini',\n                                                     max_depth=20,\n                                                     max_features=0.6,\n                                                     min_samples_split=0.1),\n    'RandomForestClassifier': RandomForestClassifier(max_depth=4,\n                                                     max_features=0.6,\n                                                     min_samples_leaf=1,\n                                                     n_estimators=50),\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#subgroup-metrics-computation","title":"Subgroup Metrics Computation","text":"<p>After the variables are input to a user interface, the interface uses subgroup analyzers to compute different sets of metrics for each privileged and disprivileged subgroup. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metrics computation, their metrics are combined, returned in a matrix format, and stored in a file if defined.</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom pymongo import MongoClient\n\n\nload_dotenv(os.path.join(ROOT_DIR, 'secrets.env'))  # Take environment variables from .env\n\n# Provide the mongodb atlas url to connect python to mongodb using pymongo\nCONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n# Create a connection using MongoClient. You can import MongoClient or use pymongo.MongoClient\nclient = MongoClient(CONNECTION_STRING)\ncollection = client[os.getenv(\"DB_NAME\")]['preprocessing_results']\n\n\ndef db_writer_func(run_models_metrics_df, collection=collection):\n    run_models_metrics_df.columns = run_models_metrics_df.columns.str.lower()  # Rename Pandas columns to lower case\n    collection.insert_many(run_models_metrics_df.to_dict('records'))\n</code></pre> <pre><code>import uuid\n\ncustom_table_fields_dct = {\n    'session_uuid': str(uuid.uuid4()),\n    'preprocessing_techniques': 'one hot encoder and scaler',\n}\nprint('Current session uuid: ', custom_table_fields_dct['session_uuid'])\n</code></pre> <pre><code>Current session uuid:  b2945b06-3cbe-4e6f-80f4-8f8b8228b359\n</code></pre> <p><pre><code>extra_test_sets_lst = [(base_flow_dataset.X_test, base_flow_dataset.y_test)]\ncompute_metrics_multiple_runs_with_multiple_test_sets(base_flow_dataset, extra_test_sets_lst, config, models_config,\n                                                      custom_table_fields_dct, db_writer_func)\n</code></pre> A lot of logs...</p> <pre><code>def read_model_metric_dfs_from_db(collection, session_uuid):\n    cursor = collection.find({'session_uuid': session_uuid})\n    records = []\n    for record in cursor:\n        del record['_id']\n        records.append(record)\n\n    model_metric_dfs = pd.DataFrame(records)\n\n    # Capitalize column names to be consistent across the whole library\n    new_column_names = []\n    for col in model_metric_dfs.columns:\n        new_col_name = '_'.join([c.capitalize() for c in col.split('_')])\n        new_column_names.append(new_col_name)\n\n    model_metric_dfs.columns = new_column_names\n    return model_metric_dfs\n</code></pre> <pre><code>model_metric_dfs = read_model_metric_dfs_from_db(collection, custom_table_fields_dct['session_uuid'])\nmodels_metrics_dct = create_models_metrics_dct_from_database_df(model_metric_dfs)\n</code></pre>"},{"location":"introduction/welcome_to_virny/","title":"Welcome to Virny","text":""},{"location":"introduction/welcome_to_virny/#description","title":"\ud83d\udcdc Description","text":"<p>Virny is a Python library for auditing model stability and fairness. The Virny library was developed based on three fundamental principles:</p> <p>1) easy extensibility of model analysis capabilities;</p> <p>2) compatibility to user-defined/custom datasets and model types;</p> <p>3) simple composition of parity metrics based on context of use.</p> <p>Virny decouples model auditing into several stages, including: subgroup metrics computation, group metrics composition, and metrics visualization and reporting. This gives data scientists and practitioners more control and flexibility to use the library for model development and monitoring post-deployment.</p> <p>For quickstart, look at our Use Case Examples.</p>"},{"location":"introduction/welcome_to_virny/#installation","title":"\ud83d\udee0 Installation","text":"<p>Virny supports Python 3.8 (recommended), 3.9 and can be installed with <code>pip</code>:</p> <pre><code>pip install virny\n</code></pre>"},{"location":"introduction/welcome_to_virny/#documentation","title":"\ud83d\udcd2 Documentation","text":"<ul> <li>Introduction</li> <li>API Reference</li> <li>Use Case Examples</li> </ul>"},{"location":"introduction/welcome_to_virny/#features","title":"\ud83d\udca1 Features","text":"<ul> <li>Entire pipeline for auditing model stability and fairness</li> <li>Metrics reports and visualizations</li> <li>Ability to analyze intersections of sensitive attributes</li> <li>Convenient metric computation interfaces: an interface for multiple models, an interface for multiple test sets, and an interface for saving results into a user-defined database</li> <li>An <code>error_analysis</code> computation mode to analyze model stability and confidence for correct and incorrect prodictions splitted by groups</li> <li>Data loaders with subsampling for fairness datasets</li> <li>User-friendly parameters input via config yaml files</li> <li>Check out our documentation for a comprehensive overview</li> </ul>"},{"location":"introduction/welcome_to_virny/#library-terminology","title":"\ud83d\udcd6 Library Terminology","text":"<p>This section briefly explains the main terminology used in our library.</p> <ul> <li>A sensitive attribute is an attribute that partitions the population into groups with unequal benefits received.</li> <li>A protected group (or simply group) is created by partitioning the population by one or many sensitive attributes.</li> <li>A privileged value of a sensitive attribute is a value that gives more benefit to a protected group, which includes it, than to protected groups, which do not include it.</li> <li>A subgroup is created by splitting a protected group by privileges and disprivileged values.</li> <li>A group metric is a metric that shows the relation between privileged and disprivileged subgroups created based on one or many sensitive attributes.</li> </ul>"},{"location":"release_notes/0.1.0/","title":"0.1.0 - 2023-02-06","text":"<ul> <li>PyPI</li> <li>GitHub</li> </ul>"},{"location":"release_notes/0.1.0/#models-audit-pipeline","title":"\ud83d\ude80 Models Audit Pipeline","text":"<ul> <li> <p>Developed an entire pipeline for auditing model stability and fairness with detailed reports and visualizations</p> </li> <li> <p>Designed and implemented an extensible architecture split on components (User interfaces, MetricsComposer, etc.) that can be easily adapted to your needs</p> </li> <li> <p>Enabled easy pipeline adaptability for different classification datasets</p> </li> <li> <p>Added a feature to audit blind classifiers, which were trained on features without sensitive attributes, and use these sensitive attributes for analysis</p> </li> </ul>"},{"location":"release_notes/0.1.0/#user-interfaces","title":"\ud83d\udc69\u200d\ud83d\udcbb User Interfaces","text":"<ul> <li> <p>Added three types of user interfaces:</p> <ul> <li>Interface for multiple runs and multiple models</li> <li>Interface for multiple models and one run</li> <li>Interface for one model and one run</li> </ul> </li> <li> <p>Added an ability to input arguments to interfaces via user-friendly config yaml files or direct arguments</p> </li> </ul>"},{"location":"release_notes/0.1.0/#datasets-and-preprocessing","title":"\ud83d\uddc3 Datasets and Preprocessing","text":"<ul> <li> <p>Added built-in preprocessing techniques for raw classification datasets</p> </li> <li> <p>Developed an ability to work with non-binary features</p> </li> <li> <p>Enabled access to COMPAS and Folktables benchmark datasets via implemented data loaders</p> </li> </ul>"},{"location":"release_notes/0.1.0/#analyzers-and-metrics","title":"\ud83d\udca0 Analyzers and Metrics","text":"<ul> <li> <p>Added an ability to analyze intersections of sensitive attributes</p> </li> <li> <p>Implemented a set of error and variance metrics:</p> <ul> <li>6 subgroup variance metrics<ul> <li>Mean</li> <li>Std</li> <li>IQR</li> <li>Entropy</li> <li>Jitter</li> <li>Label Stability</li> </ul> </li> <li>9 subgroup error metrics<ul> <li>TPR</li> <li>TNR</li> <li>PPV</li> <li>FNR</li> <li>FPR</li> <li>Accuracy</li> <li>F1</li> <li>Selection-Rate</li> <li>Positive-Rate</li> </ul> </li> <li>5 group variance metrics<ul> <li>Label Stability Ratio</li> <li>IQR Parity</li> <li>Std Parity</li> <li>Std Ratio</li> <li>Jitter Parity</li> </ul> </li> <li>5 group fairness metrics<ul> <li>Equalized Odds TPR</li> <li>Equalized Odds FPR</li> <li>Disparate Impact</li> <li>Statistical Parity Difference</li> <li>Accuracy Parity</li> </ul> </li> </ul> </li> </ul>"},{"location":"release_notes/0.1.0/#reports-and-visualizations","title":"\ud83d\udcc8 Reports and Visualizations","text":"<ul> <li> <p>Added an ability to create predefined plots for result metrics</p> </li> <li> <p>Developed a feature to make detailed summary reports with visualizations</p> </li> </ul>"},{"location":"release_notes/0.1.0/#convenience","title":"\ud83d\ude0c Convenience","text":"<ul> <li> <p>Enabled smart saving of result metrics in files</p> </li> <li> <p>In the multiple runs interface, a file with result metrics is saved each time when each run is completed. In such a way, if you get an error in one of the runs, the results of the previous runs will be saved.</p> </li> <li> <p>Enabled library installation via pip</p> </li> <li> <p>Created and hosted a website for detailed documentation with examples</p> </li> </ul>"},{"location":"release_notes/0.2.0/","title":"0.2.0 - 2023-05-15","text":"<ul> <li>PyPI</li> <li>GitHub</li> </ul>"},{"location":"release_notes/0.2.0/#user-interfaces","title":"\ud83d\udc69\u200d\ud83d\udcbb User Interfaces","text":"<ul> <li>Added two new types of user interfaces:<ul> <li>Multiple runs, multiple models with DB writer. This interface has the same functionality as the previous one,    but result metrics are stored in a user database. For that, users need to pass a DB writer function to the interface that can write result   metrics to their database. After each metrics computation run, the interface will use this function to save results in the database.</li> <li>Multiple runs, multiple models with several test sets. Except for a traditional flow   with one test set, Virny has an interface to work with many test sets that could   be used for model stress testing. This interface uses the same estimators in the bootstrap   for inference on the input test sets and saves metrics for each test set in a user   database. In such a way, a model comparison on different test sets is faster and more accurate.</li> </ul> </li> </ul>"},{"location":"release_notes/0.2.0/#datasets","title":"\ud83d\uddc3 Datasets","text":"<ul> <li>Added 5 new data loaders for all tasks in the folktables benchmark<ul> <li>ACSIncomeDataset. Dataset class for the income task from the folktables dataset.   Target: binary classification, predict if a person has an annual income &gt; $50,000.</li> <li>ACSEmploymentDataset. Dataset class for the employment task from the folktables dataset.   Target: binary classification, predict if a person is employed.</li> <li>ACSMobilityDataset. Dataset class for the mobility task from the folktables dataset.   Target: binary classification, predict whether a young adult moved addresses in the last year.</li> <li>ACSPublicCoverageDataset. Dataset class for the public coverage task from the folktables dataset.   Target: binary classification, predict whether a low-income individual, not eligible for Medicare,   has coverage from public health insurance.</li> <li>ACSTravelTimeDataset. Dataset class for the travel time task from the folktables dataset.   Target: binary classification, predict whether a working adult has a travel time to work of greater than 20 minutes.</li> </ul> </li> </ul>"},{"location":"release_notes/0.2.0/#analyzers-and-metrics","title":"\ud83d\udca0 Analyzers and Metrics","text":"<ul> <li> <p>Developed an ability to define subgroups based on a list of values, e.g., create a subgroup based on values from 30 to 45 for the age column.</p> </li> <li> <p>Extended the ability to define intersectional groups based on 3 or more columns and conditions.</p> </li> </ul>"},{"location":"release_notes/0.3.0/","title":"0.3.0 - 2023-08-14","text":"<ul> <li>PyPI</li> <li>GitHub</li> </ul>"},{"location":"release_notes/0.3.0/#new-metrics-computation-mode","title":"\u2699\ufe0f New Metrics Computation Mode","text":"<ul> <li> <p>An <code>error_analysis</code> mode that measures subgroup and group metrics for correct and incorrect predictions, in addition to default groups.  For example, when a default computation mode measures metrics for sex_priv and sex_dis, an <code>error_analysis</code> mode measures metrics  for (sex_priv, sex_priv_correct, sex_priv_incorrect) and (sex_dis, sex_dis_correct, sex_dis_incorrect).  Therefore, a user can analyze how a model is stable or certain about its incorrect predictions.</p> </li> <li> <p>An example yaml file for the computation mode: <pre><code>dataset_name: COMPAS\nbootstrap_fraction: 0.8\nn_estimators: 50\ncomputation_mode: error_analysis\nsensitive_attributes_dct: {'sex': 1, 'race': 'African-American', 'sex&amp;race': None}\n</code></pre></p> </li> </ul>"},{"location":"release_notes/0.3.0/#new-fairness-datasets","title":"\ud83d\uddc3 New Fairness Datasets","text":"<ul> <li>LawSchoolDataset. A dataset class for the Law School dataset that contains sensitive attributes among feature columns. <ul> <li>Target: predict whether a candidate would pass the bar exam or predict a student\u2019s first-year average grade (FYA).</li> <li>Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/law_school_clean.csv.</li> <li>Broader description: https://arxiv.org/pdf/2110.00530.pdf.</li> </ul> </li> <li>DiabetesDataset. A dataset class for the Diabetes dataset that contains sensitive attributes among feature columns.<ul> <li>Target: predict whether a patient will readmit within 30 days.</li> <li>Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/diabetes-clean.csv.</li> <li>Broader description: https://arxiv.org/pdf/2110.00530.pdf.</li> </ul> </li> <li>RicciDataset. A dataset class for the Ricci dataset that contains sensitive attributes among feature columns.<ul> <li>Target: predict whether an individual obtains a promotion based on the exam results.</li> <li>Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/ricci_race.csv.</li> <li>Broader description: https://arxiv.org/pdf/2110.00530.pdf.</li> </ul> </li> </ul>"},{"location":"release_notes/0.3.0/#analyzers-and-metrics","title":"\ud83d\udca0 Analyzers and Metrics","text":"<ul> <li> <p>New subgroup metrics:</p> <ul> <li>Statistical Bias is a feature of a statistical technique or of its results whereby the expected value of the results differs from the true underlying quantitative parameter being estimated (ref).</li> <li>Aleatoric Uncertainty is a mean entropy of ensemble (ref).</li> <li>Overall Uncertainty is an entropy of mean prediction of ensemble (ref).</li> </ul> </li> <li> <p>Changed a reference group in a sensitive_attributes_dct: now a disadvantaged group is used as a reference to compute intersectional metrics. For example, if we need to compute metrics for sex &amp; race group (sex -- [male, female], race -- [white, black]), then sex&amp;race_dis would include records for black females, and sex&amp;race_priv would include all other records in a dataset.</p> </li> </ul>"}]}